# libraries
import matplotlib as mpl
mpl.use('Agg')
import argparse
from pytadbit.mapping.full_mapper import full_mapping
from pytadbit.parsers.genome_parser import parse_fasta
from pytadbit.parsers.map_parser import parse_map
from pytadbit.mapping import get_intersection
from pytadbit.mapping.analyze import plot_iterative_mapping
from pytadbit.mapping.filter import filter_reads
from pytadbit.mapping.filter import apply_filter
from pytadbit.mapping.analyze import insert_sizes
from pytadbit.mapping.analyze import hic_map
from pytadbit.parsers.hic_bam_parser import bed2D_to_BAMhic
import subprocess
import os
import shutil
import copy
from pytadbit.mapping.filter import _filter_duplicates_strict
from pytadbit.mapping.filter import _filter_duplicates_loose
import multiprocessing as mu
import pickle

# functions
# MASKED_ref = {1 : {'name': 'self-circle'       , 'reads': 0},
#           2 : {'name': 'dangling-end'      , 'reads': 0},
#           3 : {'name': 'error'             , 'reads': 0},
#           4 : {'name': 'extra dangling-end', 'reads': 0},
#           5 : {'name': 'too close from RES', 'reads': 0},
#           6 : {'name': 'too short'         , 'reads': 0},
#           7 : {'name': 'too large'         , 'reads': 0},
#           8 : {'name': 'over-represented'  , 'reads': 0},
#           9 : {'name': 'duplicated'        , 'reads': 0},
#           10: {'name': 'random breaks'     , 'reads': 0},
#           11: {'name': 'trans-chromosomic' , 'reads': 0}}
# MASKED = copy.copy(MASKED_ref)


# def filter_read_duplicates(fnam, output=None, 
#                 verbose=True,
#                  savedata=None,  strict_duplicates=False):
#     """
#     Filter mapped pair of reads in order to remove experimental artifacts (e.g.
#     duplicates)
#         9- duplicated: the combination of the start positions of the
#             reads is repeated -> PCR artifact (only keep one copy)
       
#     :param fnam: path to file containing the pair of reads in tsv format, file
#         generated by :func:`pytadbit.mapping.mapper.get_intersection`
#     :param None output: PATH where to write files containing IDs of filtered
#         reads. Uses fnam by default.
#     :param None savedata: PATH where to write the number of reads retained by
#         each filter
#     :param False strict_duplicates: by default reads are considered duplicates if
#         they coincide in genomic coordinates and strand; with strict_duplicates
#         enabled, we also ask to consider read length (WARNING: this option is
#         called strict, but it is more permissive).

#     :return: dictionary with, as keys, the kind of filter applied, and as values
#         a set of read IDs to be removed

#     *Note: Filtering is not exclusive, one read can be filtered several times.*
#     """
#     if not output:
#         output = fnam
    
#     if strict_duplicates:
#         _filter_duplicates = _filter_duplicates_strict
#     else:
#         _filter_duplicates = _filter_duplicates_loose
    
#     pool = mu.Pool(1)

#     d = pool.apply_async(_filter_duplicates,
#                         args=(fnam, output))
#     pool.close()
#     pool.join()
#     sub_mask, total = d.get()
#     MASKED.update(sub_mask)

#     # if savedata or verbose:
#     #     bads = len(frozenset().union(*[masked[k]['reads'] for k in masked]))
#     if savedata:
#         out = open(savedata, 'w')
#         out.write('Mapped both\t%d\n' % total)
#         for k in range(1, len(MASKED) + 1):
#             out.write('%s\t%d\n' % (MASKED[k]['name'], MASKED[k]['reads']))
#         # out.write('Valid pairs\t%d\n' % (total - bads))
#         out.close()
#     if verbose:
#         print('Filtered reads (and percentage of total):\n')
#         print('     {:>25}  : {:12,} (100.00%)'.format('Mapped both', total))
#         print('  ' + '-' * 53)
#         for k in range(1, len(MASKED)):
#             print('  {:2}- {:>25} : {:12,} ({:6.2f}%)'.format(
#                 k, MASKED[k]['name'], MASKED[k]['reads'],
#                 float(MASKED[k]['reads']) / total * 100))
#     return MASKED

##########################################################################
## To be defined as imput
parser = argparse.ArgumentParser(description = 'Code to map ChIA-drop data')
parser.add_argument('-fq1','--fastq1', help = 'Path to fasq file of read 1',required=True)
parser.add_argument('-fq2','--fastq2', help = 'Path to fasq file of read 2', required=True)
parser.add_argument('-gnm','--genome', help = 'Path to reference genome', required=True)
parser.add_argument('-gi','--gemindex', help = 'Path to GEM index file', required=True)
parser.add_argument('-op','--outpath',help='Path to output folder', required=True)
parser.add_argument('-opt','--tempdir',help='Path for temporal output folder', required=False)
#parser.add_argument('-mins','--minseq',help='Minimum bp length to map (def. 25bp)', required=False)
#parser.add_argument('-maxs','--maxseq',help='Maximum bp length to map (Min read length)', required=True)
parser.add_argument('-renz','--renz',help='Used restriction enzyme', required=True)
parser.add_argument('-t','--threads',help='Number of CPU to use', required=False)

# prefered FASTQ naming format (all desired content to ID in name before first '_')
#path/name_extraContent.fastq.dsrc
#files = [
#    '/scratch/julen/chiadrop/testDatasets/fastq/SRR7722065-sra_S1_L001_R1_001.fastq.gz',
#    '/scratch/julen/chiadrop/testDatasets/fastq/SRR7722065-sra_S1_L001_R2_001.fastq.gz',
#
#   '/scratch/julen/chiadrop/testDatasets/fastq/SRR7722055-sra_S1_L001_R1_001.fastq.gz',
#    '/scratch/julen/chiadrop/testDatasets/fastq/SRR7722055-sra_S1_L001_R2_001.fastq.gz',
#]
args = parser.parse_args()
# get read 1 and 2 files
fi1 = args.fastq1
fi2 = args.fastq2

# load genome path
genome = args.genome
#genome = '/ssd/genomes/dm6/dm6.fa'

# load GEM index path
gem_index_path = args.gemindex
#gem_index_path = '/ssd/genomes/dm6/dm6.gem'

# get output folder
path = args.outpath
# path = '/scratch/julen/chiadrop/testDatasets/mapping/'

# get temporal output folder if present
tempOut=args.tempdir
# if we provide an alternative output dir we change it 
if tempOut == None:
    tempOut = path
    # wont need to move files from temporal to main folder
    moveFiles = False
else:
    moveFiles = True

# get restriction enzyme name
r_enz = args.renz

# minimum allowed number of bp in a range
# minseq=int(args.minseq)
# if minseq == None:
#     minseq = 25
# maximum allowed number of bp to be mapped (max length of read=)
# maxseq=int(args.maxseq)
# maxseq = 130
# if maxseq == None:
#     maxseq = 100

# get number of CPU to use
nthreads = int(args.threads)
if nthreads == None:
    # prepare for a very long journey
    nthreads = 1
#nthreads=32


###########################################################################
## code start
# get mapping ranges removing from each border
#rangePos = [(1, r) for r in range(maxseq, 25, -20)] + [(r, maxseq) for r in range(25, maxseq, 20)]
# get mapping ranges removing from both borders
#mid = [(1+s1, maxseq-s1) for s1 in range(25, maxseq, 20)]
#mid = [m for m in mid if m[0] < m[1] and m[1] - m[0] > minseq]
#rangePos += mid
#rangePos = tuple([r for r in rangePos if abs(r[0]-r[1]) >= (minseq - 1)])

# wont plot anything in case there are issues, only store
showInterMatrix=False
# if we got no reads will stop further steps
STOPnow = False

## Mapping
print(fi1)
print(fi2)
print(path)
print(tempOut)
id1 = fi1.split('/')[-1].split('_')[0]
#cell = fi1.split('_')[-4]
#rep = fi1.split('_')[-3]
max_molecule_length = {}

genomeName = genome.split('/')[-1][:-3]
# If we dont have the bam files for these experiments
cellpath = tempOut + '%s/%s/' %(genomeName, id1)
if not os.path.exists(cellpath):
    os.makedirs(cellpath)

pathFinal = f'{path}/{genomeName}/{id1}/'
if not os.path.exists(pathFinal):
    os.makedirs(pathFinal)

outKeep = tempOut + '%s/%s/03_filtering/' %(genomeName, id1)
outTSV = '{0}valid_reads12_{1}.tsv'.format(pathFinal, id1)
if not os.path.exists(outTSV):
    # create folder for cell line and replicate
    outMap = tempOut + '%s/%s/01_mapping/' %(genomeName, id1)
    # If this directory does not exist yet
    print('%s outMap' %(id1))
    if not os.path.exists(outMap):
        try:
            print('%s inMap' %(id1))
            os.makedirs(outMap)
            temp_dir1 = '{0}/mapped_{1}_r1_tmp/'.format(outMap, id1)
            os.makedirs(temp_dir1)

            temp_dir2 = '{0}/mapped_{1}_r2_tmp/'.format(outMap, id1)
            os.makedirs(temp_dir2)

            # for the first side of the reads 
            full_mapping(mapper_index_path=gem_index_path,
                         out_map_dir='{0}mapped_{1}_r1/'.format(outMap, id1),
                         fastq_path=fi1,
                         r_enz=r_enz, frag_map=True, clean=True, nthreads=nthreads, 
                         temp_dir=temp_dir1)

            # for the second side of the reads
            full_mapping(mapper_index_path=gem_index_path,
                         out_map_dir='{0}/mapped_{1}_r2/'.format(outMap, id1),
                         fastq_path=fi2,
                         r_enz=r_enz, frag_map=True, clean=True, nthreads=nthreads, 
                         temp_dir=temp_dir2)

            
        except Exception as e:
            print('Mapping failed')
            print(e)
            shutil.rmtree(outMap)
            STOPnow = True
            


    ## Parsing
    # create folder for cell line and replicate
    outPars = tempOut + '%s/%s/02_parsing/' %(genomeName, id1)
    # If this directory does not exist yet
    outKeep = tempOut + '%s/%s/03_filtering/' %(genomeName, id1)
    reads = '{0}reads12_{1}.tsv'.format(outKeep, id1)
    print('%s outParsing' %(id1))
    if not os.path.exists(reads) and STOPnow == False:
        print('%s inParsing' %(id1))
        if not os.path.exists(outPars):
            os.makedirs(outPars)

        # Load genomic sequence to map restriction sites
        genome_seq = parse_fasta(genome)

        id11 = fi1.split('/')[-1].split('.fastq')[0]
        # if running with uncompressed fastq file
        #maps1 = [f'{outMap}mapped_{id1}_r1/{id11}.fastq_full_{p1}-{p2}.map'
        #            for p1, p2 in rangePos]
        #maps2 = [f'{outMap}mapped_{id1}_r2/{id22}.fastq_full_{p1}-{p2}.map'
        #            for p1, p2 in rangePos]


        # if running with compressed files
        maps1 = [f'{outMap}mapped_{id1}_r1/{id11}_full_1-end.map',
                f'{outMap}mapped_{id1}_r1/{id11}_frag_1-end.map']


        id22 = fi2.split('/')[-1].split('.fastq')[0]
        maps2 = [f'{outMap}mapped_{id1}_r2/{id22}_full_1-end.map',
                f'{outMap}mapped_{id1}_r2/{id22}_frag_1-end.map']

        reads1 = '{0}reads1_{1}.tsv'.format(outPars, id1)
        reads2 = '{0}reads2_{1}.tsv'.format(outPars, id1)

        #
        parse_map(maps1, maps2,
                    reads1, 
                    reads2, 
                    genome_seq=genome_seq, re_name=r_enz, 
                    verbose=True)

        # esto deberia ser el numero total de reads en mi experimento
        bashCommand = f"zcat {fi1} | wc -l"
        process = subprocess.run(bashCommand, stdout=subprocess.PIPE, shell=True)
        output = process.stdout
        total_reads = int(output.rstrip()) // 4

        #fq = pyfastx.Fastq(fi1)
        #total_reads = len(fq)
        #del fq
        print(f'{total_reads} reads in the original FASTQ files')

        lengths = plot_iterative_mapping(reads1, reads2, total_reads,
                                    savefig=pathFinal+'fragMapping_%s.png' %id1
                                        )

        with open(f'{pathFinal}readsMappedLength_{id1}.pickle', 'wb') as handle:
            pickle.dump(lengths, handle, protocol=pickle.HIGHEST_PROTOCOL)

        # Keep only uniquely mapped reads and join
        outKeep = tempOut + '%s/%s/03_filtering/' %(genomeName, id1)
        if not os.path.exists(outKeep):
            os.makedirs(outKeep)
        reads = '{0}reads12_{1}.tsv'.format(outKeep, id1)

        #### IN THE FUTURE WE COULD STOP IN HERE ######
        
        
        # Keep only uniquely mapped reads
        count, _ = get_intersection(reads1, reads2, reads, verbose=True)

        if count == 0:
            print('Crap mapping, no reads from both ends survived. Stoping here')
            print(fi1)
            print(fi2)
            STOPnow = True
            
        #plot_distance_vs_interactions(reads, resolution=100000, max_diff=1000, show=True)
    if STOPnow == False:
        outKeep = tempOut + '%s/%s/03_filtering/' %(genomeName, id1)
        outTSV = '{0}valid_reads12_{1}.tsv'.format(pathFinal, id1)
        print('%s outFiltering' %(id1))
        if not os.path.exists(outTSV):
            print('%s inFiltering' %(id1))

            # Get maximum molecule length
            reads = '{0}reads12_{1}.tsv'.format(outKeep, id1)

            # Plot insert sizes
            max_molecule_length['%s' %(id1)] = insert_sizes(reads, show=False, 
                                                    nreads=1000000, savefig=pathFinal+'insertSizes_%s.png' %id1)[1]

            ## Filtering mapped reads
            min_dist_to_re = max_molecule_length['%s' %(id1)] * 1.5 
            print('max molecule length, min distance to RE')
            print(max_molecule_length['%s' %(id1)], min_dist_to_re)


            # this will last ~10 minutes
            masked = filter_reads(
                reads, 
                max_molecule_length=max_molecule_length['%s' %(id1)], over_represented=0.005, max_frag_size=100000, 
                min_frag_size=50, re_proximity=5, min_dist_to_re=min_dist_to_re)


            # Filter duplicated data
            # This is the only file (non-plot) we will store in a non temporal directory
            valid_reads = '{0}valid_reads12_{1}.tsv'.format(pathFinal, id1)
            apply_filter(reads, 
                        valid_reads, masked, 
                        filters=[1, 2, 3, 4, 9, 10])
                    

            
            
            # Evaluate whole genome changes
            ##     
            hic_map(valid_reads, 
                resolution=1000000, show=showInterMatrix, cmap='viridis',
                    savefig=pathFinal + 'interMatrix_%s.pdf' %id1)

            # Save BAM file
            outbam = f'{pathFinal}valid_reads12_{id1}'
            bed2D_to_BAMhic(valid_reads, 
                        valid=True, ncpus=nthreads, 
                        outbam=outbam, 
                        frmt='mid', masked=None)

            shutil.rmtree(outPars)
            shutil.rmtree(outMap)
            for f in os.listdir(outKeep):
                if f.startswith('reads12'):
                    os.remove(outKeep + f)

            # MASKED = copy.copy(MASKED_ref)
